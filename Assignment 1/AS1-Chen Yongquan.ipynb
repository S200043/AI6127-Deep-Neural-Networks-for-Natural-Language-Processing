{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deadly-calculation",
   "metadata": {},
   "source": [
    "# Assignment 1 - Chen Yongquan [G2002341D]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-longer",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-honduras",
   "metadata": {},
   "source": [
    "python           3.8.7  \n",
    "nltk             3.5  \n",
    "numpy            1.20.0  \n",
    "scikit-learn     0.23.2  \n",
    "scipy            1.6.0  \n",
    "sklearn-crfsuite 0.3.6  \n",
    "stanza           1.2  \n",
    "torch            1.7.1+cu110  \n",
    "\n",
    "CPU: Intel Core i7 3770K  \n",
    "GPU: NVIDIA GeForce 1080 Ti  \n",
    "OS:  Windows 10 x64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-withdrawal",
   "metadata": {},
   "source": [
    "## Import raw WNUT17 corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "distinguished-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WNUT17Reader(file):\n",
    "  corpus = []\n",
    "  with open(file, 'r', encoding='utf-8') as reader:\n",
    "    sentence  = []\n",
    "    for line in reader:\n",
    "      _ = line.strip()\n",
    "      if _ != '':\n",
    "        a, b = _.split('\\t')\n",
    "        sentence.append(tuple((a, b.split(',')[0])))\n",
    "      else:\n",
    "        corpus.append(sentence)\n",
    "        sentence = []\n",
    "  return corpus\n",
    "\n",
    "def WNUT17Writer(file, corpus):\n",
    "  with open(file, 'w', encoding='utf-8') as writer:\n",
    "    for sentence in corpus:\n",
    "      for word in sentence:\n",
    "        writer.writelines('\\t'.join(column for column in word) + '\\n')\n",
    "      writer.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17train = WNUT17Reader('wnut17train.conll')\n",
    "wnut17train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17dev = WNUT17Reader('emerging.dev.conll')\n",
    "wnut17dev[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-present",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17test = WNUT17Reader('emerging.test.annotated')\n",
    "wnut17test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faced-invention",
   "metadata": {},
   "source": [
    "## POS tagging WNUT17 corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "essential-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17train_sents = [[word for word, iob in sentence] for sentence in wnut17train]\n",
    "wnut17dev_sents = [[word for word, iob in sentence] for sentence in wnut17dev]\n",
    "wnut17test_sents = [[word for word, iob in sentence] for sentence in wnut17test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-johnson",
   "metadata": {},
   "source": [
    "###  NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sexual-collection",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "curious-carroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17train_tagged = nltk.pos_tag_sents(wnut17train_sents)\n",
    "wnut17dev_tagged = nltk.pos_tag_sents(wnut17dev_sents)\n",
    "wnut17test_tagged = nltk.pos_tag_sents(wnut17test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17train_tagged_nltk = [\n",
    "  [word + tuple([wnut17train[idx_sent][idx_word][1]]) for idx_word, word in enumerate(sentence)]\n",
    "  for idx_sent, sentence in enumerate(wnut17train_tagged)\n",
    "]\n",
    "wnut17train_tagged_nltk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17dev_tagged_nltk = [\n",
    "  [word + tuple([wnut17dev[idx_sent][idx_word][1]]) for idx_word, word in enumerate(sentence)]\n",
    "  for idx_sent, sentence in enumerate(wnut17dev_tagged)\n",
    "]\n",
    "wnut17dev_tagged_nltk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17test_tagged_nltk = [\n",
    "  [word + tuple([wnut17test[idx_sent][idx_word][1]]) for idx_word, word in enumerate(sentence)]\n",
    "  for idx_sent, sentence in enumerate(wnut17test_tagged)\n",
    "]\n",
    "wnut17test_tagged_nltk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "genuine-productivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "WNUT17Writer('wnut17train.nltk.conll', wnut17train_tagged_nltk)\n",
    "WNUT17Writer('emerging.dev.nltk.conll', wnut17dev_tagged_nltk)\n",
    "WNUT17Writer('emerging.test.nltk.annotated', wnut17test_tagged_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-victoria",
   "metadata": {},
   "source": [
    "### Stanza/Stanford CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "attractive-voltage",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 11.6MB/s]\n",
      "2021-02-01 15:44:03 INFO: Downloading default packages for language: en (English)...\n",
      "2021-02-01 15:44:04 INFO: File exists: C:\\Users\\Home\\stanza_resources\\en\\default.zip.\n",
      "2021-02-01 15:44:07 INFO: Finished downloading models and saved to C:\\Users\\Home\\stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "random-traffic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-02-01 15:44:07 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2021-02-01 15:44:07 INFO: Use device: gpu\n",
      "2021-02-01 15:44:07 INFO: Loading: tokenize\n",
      "2021-02-01 15:44:07 INFO: Loading: pos\n",
      "2021-02-01 15:44:10 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, pos', tokenize_pretokenized = True)\n",
    "wnut17train_tagged = nlp(wnut17train_sents)\n",
    "wnut17dev_tagged = nlp(wnut17dev_sents)\n",
    "wnut17test_tagged = nlp(wnut17test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-faith",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17train_tagged_stanza = [\n",
    "  [(word['text'], word['xpos'], wnut17train[idx_sent][idx_word][1]) for idx_word, word in enumerate(sentence)]\n",
    "  for idx_sent, sentence in enumerate(wnut17train_tagged.to_dict())]\n",
    "wnut17train_tagged_stanza[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17dev_tagged_stanza = [\n",
    "  [(word['text'], word['xpos'], wnut17dev[idx_sent][idx_word][1]) for idx_word, word in enumerate(sentence)]\n",
    "  for idx_sent, sentence in enumerate(wnut17dev_tagged.to_dict())]\n",
    "wnut17dev_tagged_stanza[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranking-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17test_tagged_stanza = [\n",
    "  [(word['text'], word['xpos'], wnut17test[idx_sent][idx_word][1]) for idx_word, word in enumerate(sentence)]\n",
    "  for idx_sent, sentence in enumerate(wnut17test_tagged.to_dict())]\n",
    "wnut17test_tagged_stanza[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "academic-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "WNUT17Writer('wnut17train.stanza.conll', wnut17train_tagged_stanza)\n",
    "WNUT17Writer('emerging.dev.stanza.conll', wnut17dev_tagged_stanza)\n",
    "WNUT17Writer('emerging.test.stanza.annotated', wnut17test_tagged_stanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-trust",
   "metadata": {},
   "source": [
    "# CRF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "radio-firewall",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader import ConllChunkCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "enclosed-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17 = ConllChunkCorpusReader(\n",
    "  \".\",\n",
    "  r\".*\\.conll\",\n",
    "  (\"person\", \"location\", \"corporation\", \"product\", \"creative-work\", \"group\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-poetry",
   "metadata": {},
   "source": [
    "## NLTK POS Tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "external-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents_nltk = wnut17.iob_sents('wnut17train.nltk.conll')\n",
    "dev_sents_nltk = wnut17.iob_sents('emerging.dev.nltk.conll')\n",
    "test_sents_nltk = wnut17.iob_sents('emerging.test.nltk.annotated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-rouge",
   "metadata": {},
   "source": [
    "## Stanza POS Tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "stopped-concept",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents_stanza = wnut17.iob_sents('wnut17train.stanza.conll')\n",
    "dev_sents_stanza = wnut17.iob_sents('emerging.dev.stanza.conll')\n",
    "test_sents_stanza = wnut17.iob_sents('emerging.test.stanza.annotated')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-armor",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-kingdom",
   "metadata": {},
   "source": [
    "Basic Features:\n",
    "1. Increased window size from 1 to 2\n",
    "2. Original word  \n",
    "3. Word stemming using NLTK SnowballStemmer  \n",
    "4. Word shapes including whether it is all in lower case, upper case, camel case, digits, alphabets or a mix of digits and alphabets  \n",
    "5. Suffix  \n",
    "6. POS tag. Changed \"postag[:2]\" to \"postag[0]\" as the former is mostly same as \"postag\" since POS tags are mostly within 3 characters. The latter can focus on the general type of the word (noun, adjective, adverb) using the first character.  \n",
    "\n",
    "Other tested features:\n",
    "1. Removed bias feature after tuning as performance dropped with it. Theoretically, it should provide model with more expressibility by adding biases some state feature.  \n",
    "2. Removed prefix feature as performance dropped. Seems reasonable as suffixes tends to provide more information on the word type (nouns [-man, -ry], inflected verbs [-ing], etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "parameters = OrderedDict()\n",
    "parameters['lower'] = True # Boolean variable to control lowercasing of words\n",
    "parameters['stem'] = True # Boolean variable to control stemming of words, overrides 'lower'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "canadian-norman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions of sentence representations for sequence labelling\n",
    "def word2features(sent, i):\n",
    "\n",
    "    word = sent[i][0]\n",
    "    postag = sent[i][1]\n",
    "\n",
    "    features = {\n",
    "        #'bias': 1.0,\n",
    "        'word': word,\n",
    "        'word.stem()': SnowballStemmer(\"english\").stem(word) if parameters['stem'] else word.lower(),\n",
    "        'word.isupper()': word.islower(),\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),\n",
    "        'word.isalnum()': word.isalnum(),\n",
    "        'word.isalpha()': word.isalpha(),\n",
    "        'postag': postag,\n",
    "        'postag[0]': postag[0],\n",
    "    }\n",
    "\n",
    "    if len(word) > 3:\n",
    "#         features['word[:2]'] = word[:2]\n",
    "        features['word[-2:]'] = word[-2:]\n",
    "#         features['word[:3]'] = word[:3]\n",
    "        features['word[-3:]'] = word[-3:]\n",
    "\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        postag1 = sent[i-1][1]\n",
    "        features.update({\n",
    "            '-1:word': word1,\n",
    "            '-1:word.stem()': SnowballStemmer(\"english\").stem(word1) if parameters['stem'] else word1.lower(),\n",
    "            '-1:word.islower()': word1.islower(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isdigit()': word1.isdigit(),\n",
    "            '-1:word.isalnum()': word1.isalnum(),\n",
    "            '-1:word.isalpha()': word1.isalpha(),\n",
    "            '-1:postag': postag1,\n",
    "            '-1:postag[0]': postag1[0],\n",
    "        })\n",
    "        if len(word1) > 3:\n",
    "#             features['-1:word[:2]'] = word1[:2]\n",
    "            features['-1:word[-2:]'] = word1[-2:]\n",
    "#             features['-1:word[:3]'] = word1[:3]\n",
    "            features['-1:word[-3:]'] = word1[-3:]\n",
    "        if i > 1:\n",
    "            word1 = sent[i-2][0]\n",
    "            postag1 = sent[i-2][1]\n",
    "            features.update({\n",
    "                '-2:word': word1,\n",
    "                '-2:word.stem()': SnowballStemmer(\"english\").stem(word1) if parameters['stem'] else word1.lower(),\n",
    "                '-2:word.islower()': word1.islower(),\n",
    "                '-2:word.isupper()': word1.isupper(),\n",
    "                '-2:word.istitle()': word1.istitle(),\n",
    "                '-2:word.isdigit()': word1.isdigit(),\n",
    "                '-2:word.isalnum()': word1.isalnum(),\n",
    "                '-2:word.isalpha()': word1.isalpha(),\n",
    "                '-2:postag': postag1,\n",
    "                '-2:postag[0]': postag1[0],\n",
    "            })\n",
    "            if len(word1) > 3:\n",
    "#                 features['-2:word[:2]'] = word1[:2]\n",
    "                features['-2:word[-2:]'] = word1[-2:]\n",
    "#                 features['-2:word[:3]'] = word1[:3]\n",
    "                features['-2:word[-3:]'] = word1[-3:]\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        postag1 = sent[i+1][1]\n",
    "        features.update({\n",
    "            '+1:word': word1,\n",
    "            '+1:word.stem()': SnowballStemmer(\"english\").stem(word1) if parameters['stem'] else word1.lower(),\n",
    "            '+1:word.islower()': word1.islower(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isdigit()': word1.isdigit(),\n",
    "            '+1:word.isalnum()': word1.isalnum(),\n",
    "            '+1:word.isalpha()': word1.isalpha(),\n",
    "            '+1:postag': postag1,\n",
    "            '+1:postag[0]': postag1[0],\n",
    "        })\n",
    "        if len(word1) > 3:\n",
    "#             features['+1:word[:2]'] = word1[:2]\n",
    "            features['+1:word[-2:]'] = word1[-2:]\n",
    "#             features['+1:word[:3]'] = word1[:3]\n",
    "            features['+1:word[-3:]'] = word1[-3:]\n",
    "        if i < len(sent)-2:\n",
    "            word1 = sent[i+2][0]\n",
    "            postag1 = sent[i+2][1]\n",
    "            features.update({\n",
    "                '+2:word': word1,\n",
    "                '+2:word.stem()': SnowballStemmer(\"english\").stem(word1) if parameters['stem'] else word1.lower(),\n",
    "                '+2:word.islower()': word1.islower(),\n",
    "                '+2:word.isupper()': word1.isupper(),\n",
    "                '+2:word.istitle()': word1.istitle(),\n",
    "                '+2:word.isdigit()': word1.isdigit(),\n",
    "                '+2:word.isalnum()': word1.isalnum(),\n",
    "                '+2:word.isalpha()': word1.isalpha(),\n",
    "                '+2:postag': postag1,\n",
    "                '+2:postag[0]': postag1[0],\n",
    "            })\n",
    "            if len(word1) > 3:\n",
    "#                 features['+2:word[:2]'] = word1[:2]\n",
    "                features['+2:word[-2:]'] = word1[-2:]\n",
    "#                 features['+2:word[:3]'] = word1[:3]\n",
    "                features['+2:word[-3:]'] = word1[-3:]\n",
    "    else:\n",
    "        # Features for words that are not at the end of a document\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, postag, label in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-impossible",
   "metadata": {},
   "source": [
    "#### Sample Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-glucose",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sentence = \" \".join([a for a,b,c in train_sents_nltk[0]])\n",
    "print(sample_sentence)\n",
    "print('')\n",
    "display(word2features(train_sents_nltk[0], 2))\n",
    "print('')\n",
    "display(sent2features(train_sents_nltk[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-community",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-telephone",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "atmospheric-jones",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence representations for sequence labelling\n",
    "X_train_nltk = [sent2features(s) for s in train_sents_nltk]\n",
    "y_train_nltk = [sent2labels(s) for s in train_sents_nltk]\n",
    "\n",
    "X_dev_nltk = [sent2features(s) for s in dev_sents_nltk]\n",
    "y_dev_nltk = [sent2labels(s) for s in dev_sents_nltk]\n",
    "\n",
    "X_test_nltk = [sent2features(s) for s in test_sents_nltk]\n",
    "y_test_nltk = [sent2labels(s) for s in test_sents_nltk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-champion",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_nltk[0],y_train_nltk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-kernel",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_nltk[0],y_dev_nltk[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-baseline",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_nltk[0],y_test_nltk[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-silver",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "widespread-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence representations for sequence labelling\n",
    "X_train_stanza = [sent2features(s) for s in train_sents_stanza]\n",
    "y_train_stanza = [sent2labels(s) for s in train_sents_stanza]\n",
    "\n",
    "X_dev_stanza = [sent2features(s) for s in dev_sents_stanza]\n",
    "y_dev_stanza = [sent2labels(s) for s in dev_sents_stanza]\n",
    "\n",
    "X_test_stanza = [sent2features(s) for s in test_sents_stanza]\n",
    "y_test_stanza = [sent2labels(s) for s in test_sents_stanza]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-customs",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stanza[0],y_train_stanza[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sought-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_stanza[0],y_dev_stanza[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "injured-decimal",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_stanza[0],y_test_stanza[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-right",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "warming-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn_crfsuite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "declared-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train CRF model\n",
    "crf_nltk = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "quarterly-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train CRF model\n",
    "crf_stanza = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "pending-cookbook",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 45571,
     "status": "ok",
     "timestamp": 1594609320602,
     "user": {
      "displayName": "AI3 member",
      "photoUrl": "",
      "userId": "02800686538475887838"
     },
     "user_tz": -480
    },
    "id": "gCX9S-VbRrpw",
    "outputId": "697226db-d931-4787-f9ca-b44f790519af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python38\\lib\\site-packages\\sklearn\\base.py:209: FutureWarning: From version 0.24, get_params will raise an AttributeError if a parameter cannot be retrieved as an instance attribute. Previously it would return None.\n",
      "  warnings.warn('From version 0.24, get_params will raise an '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_nltk.fit(X_train_nltk, y_train_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "simple-buddy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "executionInfo": {
     "elapsed": 45571,
     "status": "ok",
     "timestamp": 1594609320602,
     "user": {
      "displayName": "AI3 member",
      "photoUrl": "",
      "userId": "02800686538475887838"
     },
     "user_tz": -480
    },
    "id": "gCX9S-VbRrpw",
    "outputId": "697226db-d931-4787-f9ca-b44f790519af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_stanza.fit(X_train_stanza, y_train_stanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-plain",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "unlike-limitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "welsh-arctic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-location', 'I-location', 'B-group', 'B-corporation', 'B-person', 'B-creative-work', 'B-product', 'I-person', 'I-creative-work', 'I-corporation', 'I-group', 'I-product']\n"
     ]
    }
   ],
   "source": [
    "# get label set\n",
    "labels = list(crf_nltk.classes_)\n",
    "labels.remove('O')\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-mystery",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "black-fruit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score:\n",
      "0.9937942594137114\n",
      "\n",
      "Dev F1 Score:\n",
      "0.1737323793913934\n",
      "\n",
      "Test F1 Score:\n",
      "0.1528430312503639\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate CRF model\n",
    "y_train_nltk_pred = crf_nltk.predict(X_train_nltk)\n",
    "print(\"Train F1 Score:\")\n",
    "print(metrics.flat_f1_score(y_train_nltk, y_train_nltk_pred, average='weighted', labels=labels))\n",
    "print(\"\")\n",
    "\n",
    "y_dev_nltk_pred = crf_nltk.predict(X_dev_nltk)\n",
    "print(\"Dev F1 Score:\")\n",
    "print(metrics.flat_f1_score(y_dev_nltk, y_dev_nltk_pred, average='weighted', labels=labels))\n",
    "print(\"\")\n",
    "\n",
    "y_test_nltk_pred = crf_nltk.predict(X_test_nltk)\n",
    "print(\"Test F1 Score:\")\n",
    "print(metrics.flat_f1_score(y_test_nltk, y_test_nltk_pred, average='weighted', labels=labels))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "matched-torture",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "executionInfo": {
     "elapsed": 43351,
     "status": "ok",
     "timestamp": 1594609321771,
     "user": {
      "displayName": "AI3 member",
      "photoUrl": "",
      "userId": "02800686538475887838"
     },
     "user_tz": -480
    },
    "id": "l9wfAp8iRrp9",
    "outputId": "f3bf3543-2f58-416c-84d2-c830cdfa8e81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python38\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass labels=['B-corporation', 'I-corporation', 'B-creative-work', 'I-creative-work', 'B-group', 'I-group', 'B-location', 'I-location', 'B-person', 'I-person', 'B-product', 'I-product'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "c:\\program files\\python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  B-corporation      0.000     0.000     0.000        66\n",
      "  I-corporation      0.000     0.000     0.000        22\n",
      "B-creative-work      0.333     0.035     0.064       142\n",
      "I-creative-work      0.206     0.032     0.056       218\n",
      "        B-group      0.529     0.055     0.099       165\n",
      "        I-group      0.471     0.114     0.184        70\n",
      "     B-location      0.389     0.247     0.302       150\n",
      "     I-location      0.353     0.128     0.187        94\n",
      "       B-person      0.534     0.145     0.228       429\n",
      "       I-person      0.460     0.221     0.299       131\n",
      "      B-product      0.143     0.008     0.015       127\n",
      "      I-product      0.250     0.071     0.111       126\n",
      "\n",
      "      micro avg      0.411     0.103     0.165      1740\n",
      "      macro avg      0.306     0.088     0.129      1740\n",
      "   weighted avg      0.370     0.103     0.153      1740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test_nltk, y_test_nltk_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-modem",
   "metadata": {},
   "source": [
    "### Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "talented-beaver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score:\n",
      "0.9939881221190546\n",
      "\n",
      "Dev F1 Score:\n",
      "0.26772025537374367\n",
      "\n",
      "Test F1 Score:\n",
      "0.1829594225374353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate CRF model\n",
    "y_train_stanza_pred = crf_stanza.predict(X_train_stanza)\n",
    "print(\"Train F1 Score:\")\n",
    "print(metrics.flat_f1_score(y_train_stanza, y_train_stanza_pred, average='weighted', labels=labels))\n",
    "print(\"\")\n",
    "\n",
    "y_dev_stanza_pred = crf_stanza.predict(X_dev_stanza)\n",
    "print(\"Dev F1 Score:\")\n",
    "print(metrics.flat_f1_score(y_dev_stanza, y_dev_stanza_pred, average='weighted', labels=labels))\n",
    "print(\"\")\n",
    "\n",
    "y_test_stanza_pred = crf_stanza.predict(X_test_stanza)\n",
    "print(\"Test F1 Score:\")\n",
    "print(metrics.flat_f1_score(y_test_stanza, y_test_stanza_pred, average='weighted', labels=labels))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "funded-netherlands",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "executionInfo": {
     "elapsed": 43351,
     "status": "ok",
     "timestamp": 1594609321771,
     "user": {
      "displayName": "AI3 member",
      "photoUrl": "",
      "userId": "02800686538475887838"
     },
     "user_tz": -480
    },
    "id": "l9wfAp8iRrp9",
    "outputId": "f3bf3543-2f58-416c-84d2-c830cdfa8e81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  B-corporation      0.000     0.000     0.000        66\n",
      "  I-corporation      0.000     0.000     0.000        22\n",
      "B-creative-work      0.292     0.049     0.084       142\n",
      "I-creative-work      0.233     0.046     0.077       218\n",
      "        B-group      0.357     0.061     0.104       165\n",
      "        I-group      0.462     0.086     0.145        70\n",
      "     B-location      0.418     0.253     0.315       150\n",
      "     I-location      0.448     0.138     0.211        94\n",
      "       B-person      0.546     0.235     0.329       429\n",
      "       I-person      0.500     0.298     0.373       131\n",
      "      B-product      0.000     0.000     0.000       127\n",
      "      I-product      0.167     0.024     0.042       126\n",
      "\n",
      "      micro avg      0.437     0.130     0.201      1740\n",
      "      macro avg      0.285     0.099     0.140      1740\n",
      "   weighted avg      0.350     0.130     0.183      1740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test_stanza, y_test_stanza_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-acoustic",
   "metadata": {},
   "source": [
    "### Top Features for Stanza tagged corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-chancellor",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "1. We can see that the model has learnt to recognize corporation names in the training data like Twitter and Facebook.  \n",
    "   However, such memorization is useless when posed with new corporation names in the test and dev dataset where neither Twitter nor Facebook were encountered, resulting in a dismal F1-score of 0.  \n",
    "   It seems it is hard to recognize corporation names by transition features too, as the model failed to learn any meaningful transition features to aid in this task.  \n",
    "\n",
    "   ***5.244764 B-corporation word.stem():twitter***  \n",
    "   ***3.408561 B-corporation word.stem():facebook***\n",
    "\n",
    "2. Word stemming seems to provide marginally better results than just standardizing words to lowercases. F1 score improved in training and validation set, but dropped in the test set.  \n",
    "   Probably because the original model does not feature much noise removal from the dataset and inflections of the same word are represented as different state features.  \n",
    "   However, stemming also has a risk of increasing false positives and worsening precision as there are slight chances that different words may be stemmed to resemble a named entity.  \n",
    "   For instance, \"universal\" and \"university\" both stems to \"univers\" though the latter is a location while the former is not.  \n",
    "\n",
    "3. Interestingly, the model seems to have learn that the entities in WNUT17 have a low chance of appearing in the beginning or end of the sentence.  \n",
    "\n",
    "   ***5.736417 O        EOS***  \n",
    "   ***5.505679 O        BOS***\n",
    "\n",
    "4. Model has learnt that adverbs, verbs, pronouns, wh-determiners, wh-pronouns, wh-adverbs, have a low chance of being any of the entity classes based on the first character of the POS tag.  \n",
    "\n",
    "   ***2.771931 O        postag[0]:P***  \n",
    "   ***2.640906 O        postag[0]:W***  \n",
    "   ***2.590986 O        postag[0]:R***  \n",
    "   ***2.547664 O        postag[0]:V***\n",
    "\n",
    "5. Surprisingly, word[-3:]:ing nor postag:VBG is not within the top 100 positive and negative state features. This may be because they have been accounted for in postag[0]:V and the model did not learn to recognize these other similar features as non-entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "continent-proposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "5.736417 O        EOS\n",
      "5.505679 O        BOS\n",
      "5.244764 B-corporation word.stem():twitter\n",
      "3.954554 O        word[-3:]:day\n",
      "3.408561 B-corporation word.stem():facebook\n",
      "2.971273 B-product word.stem():iphon\n",
      "2.771931 O        postag[0]:P\n",
      "2.673593 B-group  -1:word.stem():vs\n",
      "2.640906 O        postag[0]:W\n",
      "2.590986 O        postag[0]:R\n",
      "2.547664 O        postag[0]:V\n",
      "2.508714 O        word.stem():rt\n",
      "2.406397 O        postag:CC\n",
      "2.384599 B-corporation word:twitter\n",
      "2.340121 O        word:RT\n",
      "2.338465 B-group  +1:word.stem():vs\n",
      "2.080372 B-location word.stem():uk\n",
      "2.028535 B-group  postag:NNPS\n",
      "2.012369 B-person word.stem():pope\n",
      "2.007176 O        postag:NFP\n",
      "1.997899 B-product word:iPhone\n",
      "1.960324 B-creative-work -1:word.stem():watch\n",
      "1.895639 O        postag:IN\n",
      "1.895639 O        postag[0]:I\n",
      "1.849964 O        word[-3:]:ary\n",
      "1.842711 B-person word.stem():taylor\n",
      "1.824931 O        word.stem():day\n",
      "1.818309 B-person word.stem():sam\n",
      "1.811222 B-person word.stem():justin\n",
      "1.795448 B-group  word.isupper()\n",
      "1.773071 O        word:Christmas\n",
      "1.766018 O        word.stem():d\n",
      "1.752152 I-person -1:postag:NN\n",
      "1.750202 B-location +2:word:Guster\n",
      "1.750202 B-location +2:word.stem():guster\n",
      "1.730019 B-person word:o.d.b.\n",
      "1.730019 B-person word.stem():o.d.b.\n",
      "1.730019 B-person word[-2:]:b.\n",
      "1.730019 B-person word[-3:]:.b.\n",
      "1.724786 B-corporation word.stem():walmart\n",
      "1.722512 O        word[-3:]:ber\n",
      "1.719789 B-location word[-2:]:ia\n",
      "1.698662 B-group  word[-2:]:ks\n",
      "1.696855 B-location word.stem():london\n",
      "1.694170 B-product word[-3:]:Pad\n",
      "1.685623 O        word.stem():vs\n",
      "1.672992 B-group  word[-3:]:ury\n",
      "1.669375 B-corporation -2:word.stem():updat\n",
      "1.665130 B-corporation word:ufc\n",
      "1.665130 B-corporation word.stem():ufc\n",
      "1.650103 B-person word[-3:]:ope\n",
      "1.639955 O        postag:VBD\n",
      "1.636028 B-location word:#Denver\n",
      "1.636028 B-location word.stem():#denver\n",
      "1.631468 B-group  -1:word:Go\n",
      "1.629225 B-corporation word:Twitter\n",
      "1.618284 B-location word:GA\n",
      "1.611186 B-product word.stem():xbox\n",
      "1.610164 O        -1:postag[0]:N\n",
      "1.606874 O        postag:,\n",
      "1.606874 O        postag[0]:,\n",
      "1.597258 B-location word[-3:]:lhi\n",
      "1.594546 B-location word:aintree\n",
      "1.594546 B-location word.stem():aintre\n",
      "1.594546 B-location -1:word:bbt\n",
      "1.594546 B-location -1:word.stem():bbt\n",
      "1.591362 B-location word:MD\n",
      "1.591362 B-location word.stem():md\n",
      "1.588322 B-product +1:word.stem():devic\n",
      "1.587341 B-corporation word:MTV\n",
      "1.587341 B-corporation word.stem():mtv\n",
      "1.583101 O        word.stem():may\n",
      "1.574188 B-location word:lhs\n",
      "1.574188 B-location word.stem():lhs\n",
      "1.572734 I-creative-work -2:word.stem():watch\n",
      "1.572134 B-person word:JFK\n",
      "1.572134 B-person word.stem():jfk\n",
      "1.564827 O        word[-3:]:ion\n",
      "1.563453 O        postag:MD\n",
      "1.563453 O        postag[0]:M\n",
      "1.557439 B-product word.stem():ipod\n",
      "1.552788 O        word.stem():christma\n",
      "1.552514 I-creative-work word.isdigit()\n",
      "1.546184 B-person -1:word.stem():silli\n",
      "1.541134 B-location word[-3:]:nia\n",
      "1.535371 B-creative-work -2:word.stem():boy\n",
      "1.533103 B-location -2:word:Greenville\n",
      "1.533103 B-location -2:word.stem():greenvill\n",
      "1.530618 B-corporation word.stem():youtub\n",
      "1.527839 O        word.stem():u\n",
      "1.523195 B-location word[-2:]:to\n",
      "1.521814 O        postag:PRP\n",
      "1.519498 O        postag:RB\n",
      "1.515319 O        word:New\n",
      "1.511025 B-person word:billy\n",
      "1.511025 B-person word.stem():billi\n",
      "1.511025 B-person -1:word:Silly\n",
      "1.510156 B-creative-work -2:word.stem():watch\n",
      "1.506411 B-location word.stem():ga\n",
      "1.487017 O        word:I\n",
      "\n",
      "Top negative:\n",
      "-0.925581 O        word[-2:]:id\n",
      "-0.926655 O        word.stem():eu\n",
      "-0.927989 O        -1:word:AND\n",
      "-0.936122 O        word[-2:]:tt\n",
      "-0.936487 O        word:lush\n",
      "-0.936487 O        word.stem():lush\n",
      "-0.938886 O        word[-2:]:na\n",
      "-0.940983 O        word.stem():jet\n",
      "-0.941279 O        +1:word[-3:]:ide\n",
      "-0.944096 B-person word[-2:]:st\n",
      "-0.944711 O        +2:word.stem():sometim\n",
      "-0.950101 O        +2:word[-2:]:ny\n",
      "-0.954903 B-person -1:word.stem():in\n",
      "-0.956794 O        word[-3:]:ell\n",
      "-0.957387 O        -1:word[-3:]:tch\n",
      "-0.961663 O        word[-3:]:ene\n",
      "-0.962464 O        word[-3:]:nny\n",
      "-0.965333 B-person word[-3:]:ter\n",
      "-0.967454 O        word[-3:]:ter\n",
      "-0.971133 B-group  -1:word.istitle()\n",
      "-0.979121 O        +2:word.stem():doe\n",
      "-0.980890 O        word[-3:]:ica\n",
      "-0.985054 O        word[-3:]:tle\n",
      "-0.988902 O        +2:word:Sometimes\n",
      "-0.992810 B-group  EOS\n",
      "-0.995258 O        +2:word[-3:]:mao\n",
      "-1.000058 O        +1:postag:POS\n",
      "-1.007990 O        word[-2:]:co\n",
      "-1.015836 O        word[-3:]:ark\n",
      "-1.017985 O        -1:word[-2:]:ux\n",
      "-1.023061 O        word[-2:]:ci\n",
      "-1.023659 O        -2:word[-3:]:ane\n",
      "-1.026169 O        word[-2:]:el\n",
      "-1.030005 B-person -1:word.istitle()\n",
      "-1.037516 O        -1:word.stem():dj\n",
      "-1.040868 O        +1:word[-3:]:old\n",
      "-1.044795 O        word[-2:]:lr\n",
      "-1.044795 O        word[-3:]:blr\n",
      "-1.050561 O        word[-3:]:yne\n",
      "-1.058539 O        +1:word[-3:]:eat\n",
      "-1.059605 B-location +2:word[-2:]:on\n",
      "-1.062674 O        word.stem():sunni\n",
      "-1.066351 O        word:green\n",
      "-1.067358 O        word[-2:]:go\n",
      "-1.075175 O        +2:word[-3:]:ens\n",
      "-1.076517 O        word[-3:]:ube\n",
      "-1.081627 B-location -2:word[-2:]:ed\n",
      "-1.089157 O        word[-2:]:ia\n",
      "-1.092019 O        word.stem():youtub\n",
      "-1.104357 O        word.isalpha()\n",
      "-1.108055 O        word.stem():doe\n",
      "-1.139975 O        +1:word[-2:]:ro\n",
      "-1.140712 O        +1:word:World\n",
      "-1.141097 O        word[-2:]:do\n",
      "-1.148816 O        +1:word.stem():fan\n",
      "-1.153931 O        +1:word.stem():vs\n",
      "-1.163250 O        word[-2:]:ey\n",
      "-1.164074 O        +1:word.stem():sign\n",
      "-1.166893 O        word.stem():eagl\n",
      "-1.172884 B-person +1:word[-2:]:ss\n",
      "-1.180451 O        +2:word:by\n",
      "-1.202924 O        +2:word.stem():come\n",
      "-1.213392 O        -1:word[-3:]:sey\n",
      "-1.219297 O        +1:word[-3:]:ice\n",
      "-1.222639 O        word[-2:]:ny\n",
      "-1.228244 O        -1:word:green\n",
      "-1.235343 O        postag:NNP\n",
      "-1.239718 B-person -1:word.stem():the\n",
      "-1.242799 O        word[-2:]:ma\n",
      "-1.255110 O        +1:word:fans\n",
      "-1.255334 O        word.isupper()\n",
      "-1.258322 O        word[-3:]:nia\n",
      "-1.267888 O        word:iTunes\n",
      "-1.269371 O        +1:word:games\n",
      "-1.271820 I-location +1:postag[0]:V\n",
      "-1.279975 O        word.stem():tumblr\n",
      "-1.283518 O        word[-3:]:ndo\n",
      "-1.300713 O        -1:word.stem():vs\n",
      "-1.305332 O        word[-2:]:mi\n",
      "-1.308681 O        -1:word.stem():silli\n",
      "-1.318334 O        -2:word.stem():#nowplay\n",
      "-1.329580 B-group  +2:postag[0]:P\n",
      "-1.344440 O        -2:word[-3:]:ity\n",
      "-1.350435 O        word.stem():uk\n",
      "-1.357803 B-location postag:NN\n",
      "-1.384537 O        -2:word.stem():big\n",
      "-1.394559 B-location -1:postag:JJ\n",
      "-1.399523 O        +1:word.stem():live\n",
      "-1.436884 O        word.stem():ipad\n",
      "-1.453123 O        +2:word:new\n",
      "-1.467015 O        -1:word.stem():green\n",
      "-1.484455 O        word[-2:]:ka\n",
      "-1.527906 O        word[-3:]:ton\n",
      "-1.562307 O        word[-3:]:Pad\n",
      "-1.695282 O        +1:word[-3:]:ked\n",
      "-1.732471 O        -2:word.stem():win\n",
      "-1.787248 O        word[-3:]:lds\n",
      "-1.850364 O        word.istitle()\n",
      "-1.922374 O        word[-3:]:oys\n",
      "-2.379379 O        word[-2:]:hi\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.6f %-8s %s\" % (weight, label, attr))\n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(crf_stanza.state_features_).most_common(100))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(crf_stanza.state_features_).most_common()[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-tamil",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-dominican",
   "metadata": {},
   "source": [
    "Using Stanza tagged dataset since it provides better F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "valuable-peoples",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 13.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=CRF(algorithm='lbfgs',\n",
       "                                 all_possible_transitions=True,\n",
       "                                 keep_tempfiles=None, max_iterations=100),\n",
       "                   n_iter=50, n_jobs=-1,\n",
       "                   param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000015B72518AC0>,\n",
       "                                        'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000015B7156EB50>},\n",
       "                   scoring=make_scorer(flat_f1_score, average=weighted, labels=['B-location', 'I-location', 'B-group', 'B-corporation', 'B-person', 'B-creative-work', 'B-product', 'I-person', 'I-creative-work', 'I-corporation', 'I-group', 'I-product']),\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_train_stanza, y_train_stanza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "thermal-forge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'c1': 0.16442900131755955, 'c2': 0.1517231507430664}\n",
      "best CV score: 0.42046694783673244\n",
      "model size: 1.78M\n"
     ]
    }
   ],
   "source": [
    "# crf = rs.best_estimator_\n",
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "italian-december",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 Score:\n",
      "0.9928338125897982\n",
      "\n",
      "Dev F1 Score:\n",
      "0.2891223909259757\n",
      "\n",
      "Test F1 Score:\n",
      "0.19060749452674997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_train_stanza_pred = crf.predict(X_train_stanza)\n",
    "print(\"Train F1 Score:\")\n",
    "print(metrics.flat_f1_score(y_train_stanza, y_train_stanza_pred, average='weighted', labels=labels))\n",
    "print(\"\")\n",
    "\n",
    "y_dev_stanza_pred = crf.predict(X_dev_stanza)\n",
    "print(\"Dev F1 Score:\")\n",
    "print(metrics.flat_f1_score(y_dev_stanza, y_dev_stanza_pred, average='weighted', labels=labels))\n",
    "print(\"\")\n",
    "\n",
    "y_test_stanza_pred = crf.predict(X_test_stanza)\n",
    "print(\"Test F1 Score:\")\n",
    "print(metrics.flat_f1_score(y_test_stanza, y_test_stanza_pred, average='weighted', labels=labels))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-effectiveness",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "An improvement of ~0.02 on dev set and ~0.01 on test set after automated hyperparameter tuning.  \n",
    "For comparison, scores before were:\n",
    "\n",
    "Train F1 Score:  \n",
    "0.9939881221190546\n",
    "\n",
    "Dev F1 Score:  \n",
    "0.26772025537374367\n",
    "\n",
    "Test F1 Score:  \n",
    "0.182959422"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "racial-syracuse",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "executionInfo": {
     "elapsed": 43351,
     "status": "ok",
     "timestamp": 1594609321771,
     "user": {
      "displayName": "AI3 member",
      "photoUrl": "",
      "userId": "02800686538475887838"
     },
     "user_tz": -480
    },
    "id": "l9wfAp8iRrp9",
    "outputId": "f3bf3543-2f58-416c-84d2-c830cdfa8e81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  B-corporation      0.000     0.000     0.000        66\n",
      "  I-corporation      0.000     0.000     0.000        22\n",
      "B-creative-work      0.304     0.049     0.085       142\n",
      "I-creative-work      0.244     0.046     0.077       218\n",
      "        B-group      0.423     0.067     0.115       165\n",
      "        I-group      0.538     0.100     0.169        70\n",
      "     B-location      0.408     0.267     0.323       150\n",
      "     I-location      0.448     0.138     0.211        94\n",
      "       B-person      0.556     0.245     0.340       429\n",
      "       I-person      0.506     0.328     0.398       131\n",
      "      B-product      0.000     0.000     0.000       127\n",
      "      I-product      0.600     0.024     0.046       126\n",
      "\n",
      "      micro avg      0.462     0.137     0.212      1740\n",
      "      macro avg      0.336     0.105     0.147      1740\n",
      "   weighted avg      0.395     0.137     0.191      1740\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test_stanza, y_test_stanza_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-variable",
   "metadata": {},
   "source": [
    "# Softmax Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-scotland",
   "metadata": {},
   "source": [
    "Code for loading checkpoints is at the bottom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "instant-masters",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "parameters['stem'] = False # Boolean variable to control stemming of words, turned off for softmax classifier as performance is worse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-learning",
   "metadata": {},
   "source": [
    "## Build Vocabulary and Label Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "split-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lower_case(x,lower=False,stem=False):\n",
    "    if stem:\n",
    "        return SnowballStemmer(\"english\").stem(x)\n",
    "    elif lower:\n",
    "        return x.lower()\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def create_dico(item_list):\n",
    "    \"\"\"\n",
    "    Create a dictionary of items from a list of list of items.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    for items in item_list:\n",
    "        for item in items:\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico\n",
    "\n",
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "def word_mapping(sentences, lower, stem):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of words, sorted by frequency.\n",
    "    \"\"\"\n",
    "    words = [[lower_case(token, lower, stem) if (lower or stem) else token for token, pos, iob in s] for s in sentences]\n",
    "    dico = create_dico(words)\n",
    "    dico['<PAD>'] = 10000001 #UNK tag for unknown words\n",
    "    dico['<UNK>'] = 10000000 #UNK tag for unknown words\n",
    "    word_to_id, id_to_word = create_mapping(dico)\n",
    "    print(\"Found %i unique words (%i in total)\" % (\n",
    "        len(dico), sum(len(x) for x in words)\n",
    "    ))\n",
    "    return dico, word_to_id, id_to_word\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
    "    \"\"\"\n",
    "    tags = [[iob for token, pos, iob in s] for s in sentences]\n",
    "    dico = create_dico(tags)\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print(\"Found %i unique named entity tags\" % len(dico))\n",
    "    return dico, tag_to_id, id_to_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "narrative-playlist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12842 unique words (62730 in total)\n",
      "Found 13 unique named entity tags\n"
     ]
    }
   ],
   "source": [
    "dico_words,word_to_id,id_to_word = word_mapping(train_sents_stanza, parameters['lower'], parameters['stem'])\n",
    "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sents_stanza)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applicable-determination",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "We remove 'O' from the set of labels used for metrics computation in order to achieve parity with the evaluation for the CRF classifier.\n",
    "Also, we are not interested in the accuracy of the model at predicting the 'O' class and in an unbalanced dataset where 'O' class words outnumber other classes,\n",
    "including it in the label set would cause the high \"accuracy\" of predicting the 'O' class of an poor model to overwhelm the low accuracy for the actual informational classes,\n",
    "resulting in a high weighted average F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_labels = list(tag_to_id.values())\n",
    "metric_labels.remove(tag_to_id['O'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-member",
   "metadata": {},
   "source": [
    "### Numericalize raw corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "lesbian-venice",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(sentences, word_to_id, tag_to_id, lower=False, stem=False):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
    "        - word indexes\n",
    "        - word char indexes\n",
    "        - tag indexes\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for s in sentences:\n",
    "        str_words = [w[0] for w in s]\n",
    "        words = [word_to_id[lower_case(w,lower,stem) if lower_case(w,lower,stem) in word_to_id else '<UNK>']\n",
    "                 for w in str_words]\n",
    "        tags = [tag_to_id[w[-1]] for w in s]\n",
    "        data.append({\n",
    "            'str_words': str_words,\n",
    "            'words': words,\n",
    "            'tags': tags,\n",
    "        })\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "concerned-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3394 / 1009 / 1287 sentences in train / dev / test.\n"
     ]
    }
   ],
   "source": [
    "train_data = prepare_dataset(\n",
    "    train_sents_stanza, word_to_id, tag_to_id, parameters['lower'], parameters['stem']\n",
    ")\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sents_stanza, word_to_id, tag_to_id, parameters['lower'], parameters['stem']\n",
    ")\n",
    "test_data = prepare_dataset(\n",
    "    test_sents_stanza, word_to_id, tag_to_id, parameters['lower'], parameters['stem']\n",
    ")\n",
    "print(\"{} / {} / {} sentences in train / dev / test.\".format(len(train_data), len(dev_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-circular",
   "metadata": {},
   "source": [
    "### Torch Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "supreme-fellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from sklearn_crfsuite import metrics\n",
    "from IPython.display import clear_output\n",
    "\n",
    "dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "everyday-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_for_window(sentence, window_size, pad_token=0):\n",
    "    return [pad_token]*window_size + sentence + [pad_token]*window_size\n",
    "\n",
    "def my_collate(data, window_size, word_to_id):\n",
    "    \"\"\"\n",
    "    For some chunk of sentences and labels\n",
    "        -add winow padding\n",
    "        -pad for lengths using pad_sequence\n",
    "        -convert our labels to one-hots\n",
    "        -return padded inputs, one-hot labels, and lengths\n",
    "    \"\"\"\n",
    "\n",
    "    # deal with input sentences as we've seen\n",
    "    window_padded = [pad_sentence_for_window(sentence['words'], window_size, word_to_id['<PAD>']) for sentence in data]\n",
    "\n",
    "    # append zeros to each list of token ids in batch so that they are all the same length\n",
    "    padded = nn.utils.rnn.pad_sequence([torch.LongTensor(t) for t in window_padded], batch_first=True)\n",
    "\n",
    "    # convert labels to one-hots\n",
    "    labels = []\n",
    "    lengths = []\n",
    "    for sentence in data:\n",
    "        lengths.append(len(sentence['tags']))\n",
    "        label = torch.zeros([len(sentence['tags']), len(id_to_tag)])\n",
    "        label[torch.arange(len(sentence['tags'])),sentence['tags']] = 1\n",
    "        labels.append(label)\n",
    "    padded_labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
    "\n",
    "    return padded.long(), padded_labels, torch.LongTensor(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-depth",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "metallic-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWordWindowClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A one-layer, binary word-window classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, vocab_size, pad_idx=0):\n",
    "        super(SoftmaxWordWindowClassifier, self).__init__()\n",
    "        \"\"\"\n",
    "        Instance variables.\n",
    "        \"\"\"\n",
    "        self.window_size = 2*config[\"half_window\"]+1\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        self.hidden_depth = config[\"hidden_depth\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        self.freeze_embeddings = config[\"freeze_embeddings\"]\n",
    "        self.dropout = config[\"dropout\"]\n",
    "        self.sigmoid = config[\"sigmoid\"]\n",
    "\n",
    "        \"\"\"\n",
    "        Embedding layer\n",
    "        -model holds an embedding for each layer in our vocab\n",
    "        -sets aside a special index in the embedding matrix for padding vector (of zeros)\n",
    "        -by default, embeddings are parameters (so gradients pass through them)\n",
    "        \"\"\"\n",
    "        self.embed_layer = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_idx)\n",
    "        if self.freeze_embeddings:\n",
    "            self.embed_layer.weight.requires_grad = False\n",
    "\n",
    "        \"\"\"\n",
    "        Hidden layer\n",
    "        -we want to map embedded word windows of dim (window_size+1)*self.embed_dim to a hidden layer.\n",
    "        -nn.Sequential allows you to efficiently specify sequentially structured models\n",
    "            -first the linear transformation is evoked on the embedded word windows\n",
    "            -next the nonlinear transformation tanh is evoked.\n",
    "        \"\"\"\n",
    "        hidden_layers = []\n",
    "        hidden_layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.window_size*self.embed_dim, self.hidden_dim),\n",
    "                nn.Sigmoid() if self.sigmoid else nn.Tanh()\n",
    "            )\n",
    "        )\n",
    "        if self.dropout:\n",
    "              hidden_layers.append(\n",
    "                  nn.Dropout(p=self.dropout)\n",
    "              )\n",
    "        for _ in range(1, self.hidden_depth):\n",
    "            hidden_layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "                    nn.Sigmoid() if self.sigmoid else nn.Tanh()\n",
    "                )\n",
    "            )\n",
    "            if self.dropout:\n",
    "                hidden_layers.append(\n",
    "                    nn.Dropout(p=self.dropout)\n",
    "            )\n",
    "        self.hidden_layer = nn.Sequential(*hidden_layers)\n",
    "\n",
    "        \"\"\"\n",
    "        Output layer\n",
    "        -we want to map elements of the output layer (of size self.hidden dim) to a number of classes.\n",
    "        \"\"\"\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.num_classes)\n",
    "\n",
    "        \"\"\"\n",
    "        Softmax\n",
    "        -The final step of the softmax classifier: mapping final hidden layer to class scores.\n",
    "        -pytorch has both logsoftmax and softmax functions (and many others)\n",
    "        -since our loss is the negative LOG likelihood, we use logsoftmax\n",
    "        -technically you can take the softmax, and take the log but PyTorch's implementation\n",
    "         is optimized to avoid numerical underflow issues.\n",
    "        \"\"\"\n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Let B:= batch_size\n",
    "            L:= window-padded sentence length\n",
    "            D:= self.embed_dim\n",
    "            S:= self.window_size\n",
    "            H:= self.hidden_dim\n",
    "\n",
    "        inputs: a (B, L) tensor of token indices\n",
    "        \"\"\"\n",
    "        B, L = inputs.size()\n",
    "\n",
    "        \"\"\"\n",
    "        Reshaping.\n",
    "        Takes in a (B, L) LongTensor\n",
    "        Outputs a (B, L~, S) LongTensor\n",
    "        \"\"\"\n",
    "        # Fist, get our word windows for each word in our input.\n",
    "        token_windows = inputs.unfold(1, self.window_size, 1)\n",
    "        _, adjusted_length, _ = token_windows.size()\n",
    "\n",
    "        # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
    "        assert token_windows.size() == (B, adjusted_length, self.window_size)\n",
    "\n",
    "        \"\"\"\n",
    "        Embedding.\n",
    "        Takes in a torch.LongTensor of size (B, L~, S)\n",
    "        Outputs a (B, L~, S, D) FloatTensor.\n",
    "        \"\"\"\n",
    "        embedded_windows = self.embed_layer(token_windows)\n",
    "\n",
    "        \"\"\"\n",
    "        Reshaping.\n",
    "        Takes in a (B, L~, S, D) FloatTensor.\n",
    "        Resizes it into a (B, L~, S*D) FloatTensor.\n",
    "        -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
    "        \"\"\"\n",
    "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "\n",
    "        \"\"\"\n",
    "        Layer 1.\n",
    "        Takes in a (B, L~, S*D) FloatTensor.\n",
    "        Resizes it into a (B, L~, H) FloatTensor\n",
    "        \"\"\"\n",
    "        layer_1 = self.hidden_layer(embedded_windows)\n",
    "\n",
    "        \"\"\"\n",
    "        Layer 2\n",
    "        Takes in a (B, L~, H) FloatTensor.\n",
    "        Resizes it into a (B, L~, 2) FloatTensor.\n",
    "        \"\"\"\n",
    "        output = self.output_layer(layer_1)\n",
    "\n",
    "        \"\"\"\n",
    "        Softmax.\n",
    "        Takes in a (B, L~, 2) FloatTensor of unnormalized class scores.\n",
    "        Outputs a (B, L~, 2) FloatTensor of (log-)normalized class scores.\n",
    "        \"\"\"\n",
    "        output = self.log_softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-reception",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dutch-advisory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(outputs, labels, lengths):\n",
    "    \"\"\"Computes negative LL loss on a batch of model predictions.\"\"\"\n",
    "    B, L, num_classes = outputs.size()\n",
    "    num_elems = lengths.sum().float()\n",
    "\n",
    "    # get only the values with non-zero labels\n",
    "    loss = outputs*labels\n",
    "\n",
    "    # rescale average\n",
    "    return -loss.sum() / num_elems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "confirmed-reducing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loss_function, optimizer, model, train_data):\n",
    "    model.train()\n",
    "    ## For each batch, we must reset the gradients\n",
    "    ## stored by the model.\n",
    "    total_loss = 0\n",
    "    f1_scores = []\n",
    "    for batch, labels, lengths in train_data:\n",
    "        # clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        # evoke model in training mode on batch\n",
    "        outputs = model.forward(batch.to(dev, non_blocking = True))\n",
    "        # compute loss w.r.t batch\n",
    "        loss = loss_function(outputs, labels.to(dev, non_blocking = True), lengths.to(dev, non_blocking = True))\n",
    "        # pass gradients back, startiing on loss value\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        pred = torch.argmax(outputs.detach() * torch.sum(labels.to(dev, non_blocking = True), dim = 2).unsqueeze(2), dim=2).cpu().numpy()\n",
    "        tgts = torch.argmax(labels, dim=2).cpu().numpy()\n",
    "        f1_scores.append(metrics.flat_f1_score(tgts, pred, average='weighted', labels=metric_labels))\n",
    "    # return the total to keep track of how you did this time around\n",
    "    return total_loss, f1_scores\n",
    "\n",
    "def validate(model, val_data):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    f1_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch, labels, lengths in val_data:\n",
    "            outputs = model.forward(batch.to(dev, non_blocking = True))\n",
    "            loss = loss_function(outputs, labels.to(dev, non_blocking = True), lengths.to(dev, non_blocking = True))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            pred = torch.argmax(outputs.detach() * torch.sum(labels.to(dev, non_blocking = True), dim = 2).unsqueeze(2), dim=2).cpu().numpy()\n",
    "            tgts = torch.argmax(labels, dim=2).cpu().numpy()\n",
    "            f1_scores.append(metrics.flat_f1_score(tgts, pred, average='weighted', labels=metric_labels))\n",
    "    return total_loss, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "invalid-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"half_window\": 2,\n",
    "          \"embed_dim\": 25,\n",
    "          \"hidden_dim\": 25,\n",
    "          \"hidden_depth\": 3,\n",
    "          \"num_classes\": len(tag_to_id),\n",
    "          \"freeze_embeddings\": False,\n",
    "          \"dropout\": False, # False/Probability[0.0-1.0]\n",
    "          \"sigmoid\": True,\n",
    "          \"batch_size\": 20,\n",
    "          \"learning_rate\": 10.0,\n",
    "          \"num_epochs\": 500\n",
    "         }\n",
    "model = SoftmaxWordWindowClassifier(config, len(word_to_id)).to(dev, non_blocking = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "virgin-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = config['learning_rate']\n",
    "num_epochs = config['num_epochs']\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience = 5, factor = 0.5, threshold = 0.00001, min_lr = 1.0, mode = 'max')\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 100, T_mult = 1, eta_min = 1.0 )\n",
    "# criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size=config[\"batch_size\"],\n",
    "                          shuffle=True,\n",
    "                          collate_fn=partial(my_collate, window_size=config[\"half_window\"], word_to_id=word_to_id),\n",
    "                          pin_memory=True)\n",
    "\n",
    "dev_loader = DataLoader(dev_data,\n",
    "                        batch_size=len(dev_data),\n",
    "                        shuffle=False,\n",
    "                        collate_fn=partial(my_collate, window_size=config[\"half_window\"], word_to_id=word_to_id),\n",
    "                        pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "f1_scores = []\n",
    "val_losses = []\n",
    "val_f1_scores = []\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss, f1_score = train_epoch(loss_function, optimizer, model, train_loader)\n",
    "    if epoch % 10 == 0 or epoch == num_epochs - 1:\n",
    "        losses.append(epoch_loss)\n",
    "        f1_scores.append(np.mean(f1_score))\n",
    "        val_loss, val_f1_score = validate(model, dev_loader)\n",
    "        val_losses.append(val_loss)\n",
    "        val_f1_score = np.mean(val_f1_score)\n",
    "        val_f1_scores.append(val_f1_score)\n",
    "        scheduler.step(val_f1_score) # ReduceLROnPlateau\n",
    "        checkpoint = {\n",
    "          \"config\" : config,\n",
    "          \"model\": model.state_dict(),\n",
    "          \"scheduler\": scheduler.state_dict(),\n",
    "          \"losses\": losses,\n",
    "          \"f1_scores\": f1_scores,\n",
    "          \"val_losses\": val_losses,\n",
    "          \"val_f1_scores\": val_f1_scores\n",
    "        }\n",
    "        torch.save(checkpoint, \"./checkpoints/epoch_\" + str(epoch) + \".pt\")\n",
    "    # scheduler.step(epoch) # CosineAnnealingWarmRestarts\n",
    "    clear_output(wait = True)\n",
    "    print(\"Epoch \" + str(epoch + 1))\n",
    "    print(scheduler.state_dict())\n",
    "    print(\"\\nLoss: \" + str(epoch_loss))\n",
    "    print(\"\\nMean F1 Score: \" + str(np.mean(f1_score)))\n",
    "    print(\"\\n{:25s}|{:25s}|{:25s}|{:25s}\".format(\"Training Losses\", \"Validation Losses\", \"Training F1 Scores\", \"Validation F1 Scores\"))\n",
    "    for i in range(len(losses)):\n",
    "      print(\"{:<25.15f}|{:<25.15f}|{:<25.15f}|{:<25.15f}\".format(losses[i], val_losses[i], f1_scores[i], val_f1_scores[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-specification",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data,\n",
    "                          batch_size=len(train_data),\n",
    "                          shuffle=False,\n",
    "                          collate_fn=partial(my_collate, window_size=2, word_to_id=word_to_id),\n",
    "                          pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seasonal-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for batch, labels, lengths in train_loader:\n",
    "      outputs = model.forward(batch.to(dev, non_blocking = True))\n",
    "      outputs *= torch.sum(labels.to(dev, non_blocking = True), dim = 2).unsqueeze(2)\n",
    "      pred = torch.argmax(outputs, dim=2)\n",
    "      tgts = torch.argmax(labels, dim=2)\n",
    "      print(\"Flat F1 Score: \" + str(metrics.flat_f1_score(tgts.cpu().numpy(), pred.cpu().numpy(), average='weighted', labels=metric_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_loader = DataLoader(dev_data,\n",
    "                        batch_size=len(dev_data),\n",
    "                        shuffle=False,\n",
    "                        collate_fn=partial(my_collate, window_size=2, word_to_id=word_to_id),\n",
    "                        pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for batch, labels, lengths in dev_loader:\n",
    "      outputs = model.forward(batch.to(dev, non_blocking = True))\n",
    "      outputs *= torch.sum(labels.to(dev, non_blocking = True), dim = 2).unsqueeze(2)\n",
    "      pred = torch.argmax(outputs, dim=2)\n",
    "      tgts = torch.argmax(labels, dim=2)\n",
    "      print(\"Flat F1 Score: \" + str(metrics.flat_f1_score(tgts.cpu().numpy(), pred.cpu().numpy(), average='weighted', labels=metric_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data,\n",
    "                         batch_size=len(test_data),\n",
    "                         shuffle=False,\n",
    "                         collate_fn=partial(my_collate, window_size=2, word_to_id=word_to_id),\n",
    "                         pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indoor-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for batch, labels, lengths in test_loader:\n",
    "      outputs = model.forward(batch.to(dev, non_blocking = True))\n",
    "      outputs *= torch.sum(labels.to(dev, non_blocking = True), dim = 2).unsqueeze(2)\n",
    "      pred = torch.argmax(outputs, dim=2)\n",
    "      tgts = torch.argmax(labels, dim=2)\n",
    "      print(\"Flat F1 Score: \" + str(metrics.flat_f1_score(tgts.cpu().numpy(), pred.cpu().numpy(), average='weighted', labels=metric_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-schedule",
   "metadata": {},
   "source": [
    "## Prediction Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-institute",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1 = \"@paulwalk It 's the view from where I 'm living for two weeks . Empire State Building = ESB . Pretty bad storm here last evening .\".split(\" \")\n",
    "sentence2 = \"Engadget 's Stephen Fry is at top of the Empire State Building\".split(\" \")\n",
    "sentence3 = \"#StarWars #TheCloneWars is rescreening in theaters in Alderwood now\".split(\" \")\n",
    "sent_id1 = [word_to_id[lower_case(w,parameters['lower'],parameters['stem']) if lower_case(w,parameters['lower'],parameters['stem']) in word_to_id else '<UNK>'] for w in sentence1]\n",
    "sent_id2 = [word_to_id[lower_case(w,parameters['lower'],parameters['stem']) if lower_case(w,parameters['lower'],parameters['stem']) in word_to_id else '<UNK>'] for w in sentence2]\n",
    "sent_id3 = [word_to_id[lower_case(w,parameters['lower'],parameters['stem']) if lower_case(w,parameters['lower'],parameters['stem']) in word_to_id else '<UNK>'] for w in sentence3]\n",
    "demo_data = [\n",
    "  {\n",
    "    \"words\": sent_id1,\n",
    "    \"tags\" : [0] * len(sent_id1)\n",
    "  },\n",
    "  {\n",
    "    \"words\": sent_id2,\n",
    "    \"tags\" : [0] * len(sent_id2)\n",
    "  },\n",
    "  {\n",
    "    \"words\": sent_id3,\n",
    "    \"tags\" : [0] * len(sent_id3)\n",
    "  }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_loader = DataLoader(demo_data,\n",
    "                         batch_size=len(demo_data),\n",
    "                         shuffle=False,\n",
    "                         collate_fn=partial(my_collate, window_size=2, word_to_id=word_to_id),\n",
    "                         pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "japanese-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  model.eval()\n",
    "  for batch, labels, lengths in demo_loader:\n",
    "      outputs = model.forward(batch.to(dev, non_blocking = True))\n",
    "      # use generated mask instead of labels in case of test data where labels are not available\n",
    "      mask = torch.zeros([outputs.shape[0],outputs.shape[1]]).to(dev, non_blocking = True)\n",
    "      for i in range(len(lengths)):\n",
    "        mask[i,0:lengths[i]] = 1\n",
    "      mask = mask.unsqueeze(2)\n",
    "      outputs *= mask\n",
    "      # outputs *= torch.sum(labels.to(dev, non_blocking = True), dim = 2).unsqueeze(2)\n",
    "      pred = torch.argmax(outputs, dim=2)\n",
    "      tgts = torch.argmax(labels, dim=2)\n",
    "      pred_tag = [[id_to_tag[id_] for id_ in sent] for sent in pred.cpu().numpy()]\n",
    "      tgts_tag = [[id_to_tag[id_] for id_ in sent] for sent in tgts.cpu().numpy()]\n",
    "      for i, j in enumerate([*zip(pred_tag, tgts_tag)]):\n",
    "        print(\"Sentence {}:\".format(i+1))\n",
    "        print(re.sub(\"<PAD>\", \"\", \" \".join([id_to_word[id_] for id_ in batch[i].numpy()])).strip())\n",
    "        print(\"Prediction: \")\n",
    "        print(j[0][:lengths[i]])\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-fellow",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-texas",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"./checkpoints/epoch_499.pt\")\n",
    "config = checkpoint['config']\n",
    "losses = checkpoint['losses']\n",
    "f1_scores = checkpoint['f1_scores']\n",
    "val_losses = checkpoint['val_losses']\n",
    "val_f1_scores = checkpoint['val_f1_scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caring-wound",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(checkpoint['model'])\n",
    "scheduler.load_state_dict(checkpoint['scheduler'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-belief",
   "metadata": {},
   "source": [
    "## Residual Learning [Experimental]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, sigmoid):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.ll = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.sigmoid = nn.Sigmoid() if sigmoid else nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.ll(x)\n",
    "        out += identity\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "class SoftmaxWordWindowResidualClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A one-layer, binary word-window classifier.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, vocab_size, pad_idx=0):\n",
    "        super(SoftmaxWordWindowResidualClassifier, self).__init__()\n",
    "        \"\"\"\n",
    "        Instance variables.\n",
    "        \"\"\"\n",
    "        self.window_size = 2*config[\"half_window\"]+1\n",
    "        self.embed_dim = config[\"embed_dim\"]\n",
    "        self.hidden_dim = config[\"hidden_dim\"]\n",
    "        self.hidden_depth = config[\"hidden_depth\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        self.freeze_embeddings = config[\"freeze_embeddings\"]\n",
    "        self.dropout = config[\"dropout\"]\n",
    "        self.sigmoid = config[\"sigmoid\"]\n",
    "\n",
    "        \"\"\"\n",
    "        Embedding layer\n",
    "        -model holds an embedding for each layer in our vocab\n",
    "        -sets aside a special index in the embedding matrix for padding vector (of zeros)\n",
    "        -by default, embeddings are parameters (so gradients pass through them)\n",
    "        \"\"\"\n",
    "        self.embed_layer = nn.Embedding(vocab_size, self.embed_dim, padding_idx=pad_idx)\n",
    "        if self.freeze_embeddings:\n",
    "            self.embed_layer.weight.requires_grad = False\n",
    "\n",
    "        \"\"\"\n",
    "        Hidden layer\n",
    "        -we want to map embedded word windows of dim (window_size+1)*self.embed_dim to a hidden layer.\n",
    "        -nn.Sequential allows you to efficiently specify sequentially structured models\n",
    "            -first the linear transformation is evoked on the embedded word windows\n",
    "            -next the nonlinear transformation tanh is evoked.\n",
    "        \"\"\"\n",
    "        hidden_layers = []\n",
    "\n",
    "        hidden_layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.window_size*self.embed_dim, self.hidden_dim),\n",
    "                nn.Sigmoid() if self.sigmoid else nn.Tanh()\n",
    "            )\n",
    "        )\n",
    "        if self.dropout:\n",
    "              hidden_layers.append(\n",
    "                  nn.Dropout(p=self.dropout)\n",
    "              )\n",
    "        for _ in range(1, self.hidden_depth):\n",
    "            hidden_layers.append(\n",
    "                ResidualBlock(self.hidden_dim, self.sigmoid)\n",
    "            )\n",
    "            if self.dropout:\n",
    "                hidden_layers.append(\n",
    "                    nn.Dropout(p=self.dropout)\n",
    "            )\n",
    "        self.hidden_layer = nn.Sequential(*hidden_layers)\n",
    "\n",
    "        \"\"\"\n",
    "        Output layer\n",
    "        -we want to map elements of the output layer (of size self.hidden dim) to a number of classes.\n",
    "        \"\"\"\n",
    "        self.output_layer = nn.Linear(self.hidden_dim, self.num_classes)\n",
    "\n",
    "        \"\"\"\n",
    "        Softmax\n",
    "        -The final step of the softmax classifier: mapping final hidden layer to class scores.\n",
    "        -pytorch has both logsoftmax and softmax functions (and many others)\n",
    "        -since our loss is the negative LOG likelihood, we use logsoftmax\n",
    "        -technically you can take the softmax, and take the log but PyTorch's implementation\n",
    "         is optimized to avoid numerical underflow issues.\n",
    "        \"\"\"\n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Let B:= batch_size\n",
    "            L:= window-padded sentence length\n",
    "            D:= self.embed_dim\n",
    "            S:= self.window_size\n",
    "            H:= self.hidden_dim\n",
    "\n",
    "        inputs: a (B, L) tensor of token indices\n",
    "        \"\"\"\n",
    "        B, L = inputs.size()\n",
    "\n",
    "        \"\"\"\n",
    "        Reshaping.\n",
    "        Takes in a (B, L) LongTensor\n",
    "        Outputs a (B, L~, S) LongTensor\n",
    "        \"\"\"\n",
    "        # Fist, get our word windows for each word in our input.\n",
    "        token_windows = inputs.unfold(1, self.window_size, 1)\n",
    "        _, adjusted_length, _ = token_windows.size()\n",
    "\n",
    "        # Good idea to do internal tensor-size sanity checks, at the least in comments!\n",
    "        assert token_windows.size() == (B, adjusted_length, self.window_size)\n",
    "\n",
    "        \"\"\"\n",
    "        Embedding.\n",
    "        Takes in a torch.LongTensor of size (B, L~, S)\n",
    "        Outputs a (B, L~, S, D) FloatTensor.\n",
    "        \"\"\"\n",
    "        embedded_windows = self.embed_layer(token_windows)\n",
    "\n",
    "        \"\"\"\n",
    "        Reshaping.\n",
    "        Takes in a (B, L~, S, D) FloatTensor.\n",
    "        Resizes it into a (B, L~, S*D) FloatTensor.\n",
    "        -1 argument \"infers\" what the last dimension should be based on leftover axes.\n",
    "        \"\"\"\n",
    "        embedded_windows = embedded_windows.view(B, adjusted_length, -1)\n",
    "\n",
    "        \"\"\"\n",
    "        Layer 1.\n",
    "        Takes in a (B, L~, S*D) FloatTensor.\n",
    "        Resizes it into a (B, L~, H) FloatTensor\n",
    "        \"\"\"\n",
    "        layer_1 = self.hidden_layer(embedded_windows)\n",
    "\n",
    "        \"\"\"\n",
    "        Layer 2\n",
    "        Takes in a (B, L~, H) FloatTensor.\n",
    "        Resizes it into a (B, L~, 2) FloatTensor.\n",
    "        \"\"\"\n",
    "        output = self.output_layer(layer_1)\n",
    "\n",
    "        \"\"\"\n",
    "        Softmax.\n",
    "        Takes in a (B, L~, 2) FloatTensor of unnormalized class scores.\n",
    "        Outputs a (B, L~, 2) FloatTensor of (log-)normalized class scores.\n",
    "        \"\"\"\n",
    "        output = self.log_softmax(output)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
